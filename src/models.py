# -*- coding: utf-8 -*-
"""models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ePoIu-QE4NeyTqPwj3k429LmXjK7il9E
"""

# models.py
"""
Centralized definitions for all neural network architectures.
"""
import torch
import torch.nn as nn
import torch.nn.functional as F

try:
    from torch_geometric.nn import GINConv, GATConv, global_mean_pool, global_max_pool
except ImportError:
    print("Warning: torch_geometric not found. GNN models will not be available.")
    GINConv, GATConv, global_mean_pool, global_max_pool = None, None, None, None

# ... (SelfiesGenerator, ValueNetwork, and GNNClassifier classes remain unchanged) ...
class SelfiesGenerator(nn.Module):
    """SELFIES-based RNN Generator (Policy Network)."""
    def __init__(self, vocab_size: int, embed_size: int = 128,
                 hidden_size: int = 512, num_layers: int = 2, dropout: float = 0.3):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, embed_size, padding_idx=0)
        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers,
                           batch_first=True, dropout=dropout if num_layers > 1 else 0)
        self.out = nn.Linear(hidden_size, vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, hidden=None):
        embedded = self.dropout(self.emb(x))
        lstm_out, hidden = self.rnn(embedded, hidden)
        logits = self.out(self.dropout(lstm_out))
        return logits, hidden

    def get_log_probs(self, sequences: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        logits, _ = self.forward(sequences)
        log_probs = F.log_softmax(logits, dim=-1)
        target_log_probs = log_probs.gather(2, targets.unsqueeze(-1)).squeeze(-1)
        mask = (targets != 0).float()
        return (target_log_probs * mask).sum(dim=1)

class ValueNetwork(nn.Module):
    """Value network for PPO advantage estimation."""
    def __init__(self, vocab_size: int, embed_size: int = 128,
                 hidden_size: int = 512, num_layers: int = 2):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, embed_size, padding_idx=0)
        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.value_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size // 2, 1)
        )

    def forward(self, x):
        embedded = self.emb(x)
        _, (h_n, _) = self.rnn(embedded)
        value = self.value_head(h_n[-1])
        return value.squeeze(-1)

class GNNClassifier(nn.Module):
    """The GNN+ChemBERTa Reward Model Architecture."""
    def __init__(self, atom_dim: int, descriptor_dim: int, chemberta_dim: int = 768, hidden_dim: int = 128):
        super().__init__()
        if GATConv is None: raise ImportError("torch_geometric is required.")
        self.conv1 = GATConv(atom_dim, hidden_dim, heads=4, dropout=0.1)
        self.bn1 = nn.BatchNorm1d(hidden_dim * 4)
        self.conv2 = GATConv(hidden_dim * 4, hidden_dim, heads=1, dropout=0.1)
        self.bn2 = nn.BatchNorm1d(hidden_dim)
        self.chemberta_proj = nn.Linear(chemberta_dim, hidden_dim)
        self.descriptor_proj = nn.Linear(descriptor_dim, hidden_dim)
        final_dim = hidden_dim * 3
        self.classifier = nn.Sequential(
            nn.Linear(final_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim // 2), nn.ReLU(), nn.Dropout(0.2),
            nn.Linear(hidden_dim // 2, 1)
        )
    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        x = F.elu(self.bn1(self.conv1(x, edge_index)))
        x = F.elu(self.bn2(self.conv2(x, edge_index)))
        graph_pool = torch.cat([global_mean_pool(x, batch), global_max_pool(x, batch)], dim=1)
        chemberta_embed = F.elu(self.chemberta_proj(data.graph_x))
        desc_embed = F.elu(self.descriptor_proj(data.mol_descriptors))
        combined = torch.cat([graph_pool, chemberta_embed, desc_embed], dim=1)
        return self.classifier(combined)

# --- CORRECTED GNN EMBEDDER CLASS (v2) ---
class GNNEmbedder(nn.Module):
    """The GNN part of the hybrid model, corrected to use GINConv with no final projection."""
    def __init__(self, atom_dim: int, hidden_dim: int = 128):
        super().__init__()
        if GINConv is None:
            raise ImportError("torch_geometric is required to use GNNEmbedder.")

        # Define the MLP for the GIN layers
        gin_nn1 = nn.Sequential(
            nn.Linear(atom_dim, hidden_dim), # Takes 2 features
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        gin_nn2 = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

        self.conv1 = GINConv(gin_nn1)
        self.conv2 = GINConv(gin_nn2)

        # NO final projection layer

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch

        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))

        # The output of the pooling layer is the final embedding
        embedding = global_mean_pool(x, batch)
        return embedding