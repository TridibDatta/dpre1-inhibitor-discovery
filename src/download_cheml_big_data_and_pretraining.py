# -*- coding: utf-8 -*-
"""download_cheml_big_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fH-NjQ84_fv4gZ91jhhda_aVtQ-j-uk6
"""

!pip install rdkit
!pip install tqdm

import pandas as pd
import requests
from tqdm import tqdm
import gzip
import os

# --- Download the ChEMBL SMILES data ---

url = "https://ftp.ebi.ac.uk/pub/databases/chembl/ChEMBLdb/latest/chembl_35_chemreps.txt.gz"
gz_file_path = "chembl_35_chemreps.txt.gz"
txt_file_path = "chembl_35_chemreps.txt"

print(f" Downloading data from {url}...")

# Use requests with streaming to handle large files and show progress
try:
    with requests.get(url, stream=True) as r:
        r.raise_for_status()
        total_size_in_bytes = int(r.headers.get('content-length', 0))
        block_size = 1024 # 1 Kibibyte
        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)
        with open(gz_file_path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=block_size):
                progress_bar.update(len(chunk))
                f.write(chunk)
        progress_bar.close()

    print(f" Download complete. File saved as {gz_file_path}")

    # --- Step 2: Decompress the .gz file ---
    print(f" Decompressing {gz_file_path}...")
    with gzip.open(gz_file_path, 'rb') as f_in:
        with open(txt_file_path, 'wb') as f_out:
            f_out.write(f_in.read())
    print(f" Decompression complete. Unpacked file is {txt_file_path}")
    os.remove(gz_file_path) # Clean up the compressed file

    # --- Step 3: Load the SMILES strings into a Python list ---
    # The file is tab-separated with: ChEMBL_ID, SMILES, InChI, InChIKey
    print(" C—á–∏—Ç—ã–≤–∞—é –¥–∞–Ω–Ω—ã–µ –≤ DataFrame...")
    # We only need the second column (SMILES)
    df = pd.read_csv(txt_file_path, sep='\t', header=0, usecols=['canonical_smiles'])
    smiles_list = df['canonical_smiles'].dropna().astype(str).tolist()

    print(f" Successfully loaded {len(smiles_list):,} molecules.")

    # --- Step 4: (Optional but Recommended) Basic Filtering ---
    # This helps create a cleaner dataset for training
    print("\n Performing basic filtering...")
    from rdkit import Chem

    filtered_smiles = []
    for smiles in tqdm(smiles_list, desc="Filtering molecules"):
        mol = Chem.MolFromSmiles(smiles)
        if mol is not None:
            # Keep molecules with a reasonable number of heavy atoms (e.g., 5 to 50)
            if 5 < mol.GetNumHeavyAtoms() < 50:
                filtered_smiles.append(smiles)

    print(f" Filtering complete. Kept {len(filtered_smiles):,} molecules.")

    # You are now ready to use this list for pre-training!
    print(f"\nExample SMILES: {filtered_smiles[:5]}")


except Exception as e:
    print(f"An error occurred: {e}")
    print("Please check the URL or your internet connection.")

!pip install selfies rdkit

!pip install transformers torch numpy pandas scikit-learn tqdm

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from tqdm import tqdm
import random
import pickle
from rdkit import Chem
from typing import List, Tuple, Dict, Optional
import matplotlib.pyplot as plt

# Fix for cuDNN RNN issues - disable cuDNN for RNNs
torch.backends.cudnn.enabled = False
print(" Disabled cuDNN to avoid RNN training issues")

# Set device
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {DEVICE}")

# ================== DATA PROCESSING ==================

def load_and_filter_chembl(file_path: str, max_length: int = 150, min_length: int = 6) -> List[str]:
    """
    Load and filter ChEMBL SMILES data.

    Args:
        file_path: Path to ChEMBL file
        max_length: Maximum SMILES length
        min_length: Minimum SMILES length

    Returns:
        List of filtered SMILES strings
    """
    print(" Loading ChEMBL dataset...")

    try:
        # Try reading as CSV first
        df = pd.read_csv(file_path, sep='\t', low_memory=False)
        smiles_col = None

        # Find SMILES column
        for col in df.columns:
            if 'smiles' in col.lower() or 'canonical_smiles' in col.lower():
                smiles_col = col
                break

        if smiles_col is None:
            # If no header, assume first column is SMILES
            smiles_list = df.iloc[:, 0].dropna().tolist()
        else:
            smiles_list = df[smiles_col].dropna().tolist()

    except:
        # If CSV reading fails, try line-by-line
        with open(file_path, 'r') as f:
            smiles_list = [line.strip() for line in f if line.strip()]

    print(f"üìä Loaded {len(smiles_list)} SMILES strings")

    # Filter SMILES
    print("üîç Filtering SMILES...")
    filtered_smiles = []

    for smiles in tqdm(smiles_list[:100000], desc="Filtering"):  # Limit for memory
        if len(smiles) < min_length or len(smiles) > max_length:
            continue

        # Validate SMILES
        try:
            mol = Chem.MolFromSmiles(smiles)
            if mol is None:
                continue
            # Canonicalize
            canonical = Chem.MolToSmiles(mol, canonical=True)
            if canonical:
                filtered_smiles.append(canonical)
        except:
            continue

    print(f" Filtered to {len(filtered_smiles)} valid SMILES")
    return filtered_smiles

def create_vocabulary(smiles_list: List[str]) -> Tuple[Dict[str, int], Dict[int, str]]:
    """Create character-level vocabulary from SMILES."""

    print(" Creating vocabulary...")

    # Collect all unique characters
    chars = set()
    for smiles in smiles_list:
        chars.update(smiles)

    # Add special tokens
    special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']
    all_chars = sorted(list(chars)) + special_tokens

    char_to_idx = {char: idx for idx, char in enumerate(all_chars)}
    idx_to_char = {idx: char for char, idx in char_to_idx.items()}

    print(f" Vocabulary size: {len(char_to_idx)}")
    print(f" Characters: {sorted(chars)}")

    return char_to_idx, idx_to_char

# ================== DATASET CLASS ==================

class SMILESDataset(Dataset):
    """Dataset for SMILES character-level language modeling."""

    def __init__(self, smiles_list: List[str], char_to_idx: Dict[str, int],
                 max_length: int = 150):
        self.smiles_list = smiles_list
        self.char_to_idx = char_to_idx
        self.max_length = max_length

        # Special token indices
        self.sos_idx = char_to_idx['<SOS>']
        self.eos_idx = char_to_idx['<EOS>']
        self.pad_idx = char_to_idx['<PAD>']
        self.unk_idx = char_to_idx['<UNK>']

    def __len__(self):
        return len(self.smiles_list)

    def __getitem__(self, idx):
        smiles = self.smiles_list[idx]

        # Convert to indices
        indices = [self.sos_idx]
        for char in smiles:
            indices.append(self.char_to_idx.get(char, self.unk_idx))
        indices.append(self.eos_idx)

        # Pad or truncate
        if len(indices) > self.max_length:
            indices = indices[:self.max_length]
        else:
            indices.extend([self.pad_idx] * (self.max_length - len(indices)))

        # Input and target (shifted by one)
        input_seq = torch.tensor(indices[:-1], dtype=torch.long)
        target_seq = torch.tensor(indices[1:], dtype=torch.long)

        return input_seq, target_seq

# ================== MODEL ARCHITECTURE ==================

class CharRNN(nn.Module):
    """Character-level RNN for SMILES generation."""

    def __init__(self, vocab_size: int, embed_size: int = 256,
                 hidden_size: int = 512, num_layers: int = 3,
                 dropout: float = 0.3):
        super().__init__()

        self.vocab_size = vocab_size
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)

        # LSTM layers - fix for cuDNN issue
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers,
                           batch_first=True, dropout=dropout if num_layers > 1 else 0,
                           proj_size=0)  # Ensure no projection to avoid cuDNN issues

        # Output layer
        self.dropout = nn.Dropout(dropout)
        self.output = nn.Linear(hidden_size, vocab_size)

        # Initialize weights
        self._init_weights()

    def _init_weights(self):
        """Initialize model weights."""
        for name, param in self.named_parameters():
            if 'weight' in name and param.dim() > 1:
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.zeros_(param)

    def forward(self, x, hidden=None):
        """
        Forward pass with proper training mode handling.

        Args:
            x: Input tensor [batch_size, seq_len]
            hidden: Hidden state tuple (h, c)

        Returns:
            logits: [batch_size, seq_len, vocab_size]
            hidden: Updated hidden state
        """
        # Ensure model is in correct mode for cuDNN
        if self.training:
            self.lstm.train()
        else:
            self.lstm.eval()

        # Embedding
        embedded = self.embedding(x)  # [B, L, E]

        # LSTM with error handling
        try:
            lstm_out, hidden = self.lstm(embedded, hidden)  # [B, L, H]
        except RuntimeError as e:
            if "cudnn RNN backward can only be called in training mode" in str(e):
                # Force training mode for LSTM
                self.lstm.train()
                lstm_out, hidden = self.lstm(embedded, hidden)
            else:
                raise e

        # Dropout and output projection
        dropped = self.dropout(lstm_out)
        logits = self.output(dropped)  # [B, L, V]

        return logits, hidden

    def sample(self, start_token: int, eos_token: int, max_length: int = 150,
               temperature: float = 1.0, device: str = 'cpu') -> Tuple[List[int], float]:
        """
        Sample a sequence from the model.

        Args:
            start_token: Starting token index
            eos_token: End-of-sequence token index
            max_length: Maximum sequence length
            temperature: Sampling temperature
            device: Device to run on

        Returns:
            Tuple of (sequence, log_probability)
        """
        # Properly set eval mode for sampling
        self.eval()
        self.lstm.eval()

        with torch.no_grad():
            sequence = [start_token]
            hidden = None
            log_prob = 0.0

            for _ in range(max_length - 1):
                # Current input
                x = torch.tensor([[sequence[-1]]], dtype=torch.long, device=device)

                # Forward pass - ensure eval mode
                try:
                    logits, hidden = self.forward(x, hidden)
                except RuntimeError as e:
                    if "cudnn RNN backward can only be called in training mode" in str(e):
                        # Fallback: temporarily set to train mode
                        self.train()
                        self.lstm.train()
                        logits, hidden = self.forward(x, hidden)
                        self.eval()
                        self.lstm.eval()
                    else:
                        raise e

                logits = logits[0, -1, :] / temperature

                # Sample next token
                probs = F.softmax(logits, dim=-1)
                next_token = torch.multinomial(probs, 1).item()

                # Add to sequence and log probability
                sequence.append(next_token)
                log_prob += torch.log(probs[next_token]).item()

                # Stop if EOS token
                if next_token == eos_token:
                    break

        return sequence, log_prob

# Alternative: Simple RNN implementation that doesn't use cuDNN
class SimpleRNN(nn.Module):
    """Simple RNN implementation without cuDNN dependencies."""

    def __init__(self, vocab_size: int, embed_size: int = 256,
                 hidden_size: int = 512, num_layers: int = 2,
                 dropout: float = 0.3):
        super().__init__()

        self.vocab_size = vocab_size
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)

        # Manual RNN layers using Linear transformations
        self.rnn_layers = nn.ModuleList()
        for i in range(num_layers):
            input_size = embed_size if i == 0 else hidden_size
            self.rnn_layers.append(nn.Linear(input_size + hidden_size, hidden_size))

        # Output layer
        self.dropout = nn.Dropout(dropout)
        self.output = nn.Linear(hidden_size, vocab_size)

        # Initialize weights
        self._init_weights()

    def _init_weights(self):
        """Initialize model weights."""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, 0, 0.1)

    def forward(self, x, hidden=None):
        """Forward pass through manual RNN."""
        batch_size, seq_len = x.shape

        # Initialize hidden states if not provided
        if hidden is None:
            hidden = [torch.zeros(batch_size, self.hidden_size, device=x.device)
                     for _ in range(self.num_layers)]

        # Embedding
        embedded = self.embedding(x)  # [B, L, E]

        outputs = []

        # Process each timestep
        for t in range(seq_len):
            inp = embedded[:, t, :]  # [B, E]

            # Pass through RNN layers
            for layer_idx, rnn_layer in enumerate(self.rnn_layers):
                h_prev = hidden[layer_idx]
                combined = torch.cat([inp, h_prev], dim=1)  # [B, input_size + hidden_size]
                h_new = torch.tanh(rnn_layer(combined))  # [B, H]
                hidden[layer_idx] = h_new
                inp = h_new  # Input to next layer

            outputs.append(hidden[-1])  # Use output from last layer

        # Stack outputs
        rnn_out = torch.stack(outputs, dim=1)  # [B, L, H]

        # Apply dropout and output projection
        dropped = self.dropout(rnn_out)
        logits = self.output(dropped)  # [B, L, V]

        return logits, hidden

    def sample(self, start_token: int, eos_token: int, max_length: int = 150,
               temperature: float = 1.0, device: str = 'cpu') -> Tuple[List[int], float]:
        """Sample a sequence from the model."""
        self.eval()

        with torch.no_grad():
            sequence = [start_token]
            hidden = None
            log_prob = 0.0

            for _ in range(max_length - 1):
                # Current input
                x = torch.tensor([[sequence[-1]]], dtype=torch.long, device=device)

                # Forward pass
                logits, hidden = self.forward(x, hidden)
                logits = logits[0, -1, :] / temperature

                # Sample next token
                probs = F.softmax(logits, dim=-1)
                next_token = torch.multinomial(probs, 1).item()

                # Add to sequence and log probability
                sequence.append(next_token)
                log_prob += torch.log(probs[next_token] + 1e-10).item()

                # Stop if EOS token
                if next_token == eos_token:
                    break

        return sequence, log_prob
class CharGRU(nn.Module):
    """Character-level GRU for SMILES generation - more stable alternative."""

    def __init__(self, vocab_size: int, embed_size: int = 256,
                 hidden_size: int = 512, num_layers: int = 3,
                 dropout: float = 0.3):
        super().__init__()

        self.vocab_size = vocab_size
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)

        # GRU layers (more stable than LSTM with cuDNN)
        self.gru = nn.GRU(embed_size, hidden_size, num_layers,
                         batch_first=True, dropout=dropout if num_layers > 1 else 0)

        # Output layer
        self.dropout = nn.Dropout(dropout)
        self.output = nn.Linear(hidden_size, vocab_size)

        # Initialize weights
        self._init_weights()

    def _init_weights(self):
        """Initialize model weights."""
        for name, param in self.named_parameters():
            if 'weight' in name and param.dim() > 1:
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.zeros_(param)

    def forward(self, x, hidden=None):
        """Forward pass."""
        # Embedding
        embedded = self.embedding(x)  # [B, L, E]

        # GRU
        gru_out, hidden = self.gru(embedded, hidden)  # [B, L, H]

        # Dropout and output projection
        dropped = self.dropout(gru_out)
        logits = self.output(dropped)  # [B, L, V]

        return logits, hidden

    def sample(self, start_token: int, eos_token: int, max_length: int = 150,
               temperature: float = 1.0, device: str = 'cpu') -> Tuple[List[int], float]:
        """Sample a sequence from the model."""
        self.eval()

        with torch.no_grad():
            sequence = [start_token]
            hidden = None
            log_prob = 0.0

            for _ in range(max_length - 1):
                # Current input
                x = torch.tensor([[sequence[-1]]], dtype=torch.long, device=device)

                # Forward pass
                logits, hidden = self.forward(x, hidden)
                logits = logits[0, -1, :] / temperature

                # Sample next token
                probs = F.softmax(logits, dim=-1)
                next_token = torch.multinomial(probs, 1).item()

                # Add to sequence and log probability
                sequence.append(next_token)
                log_prob += torch.log(probs[next_token]).item()

                # Stop if EOS token
                if next_token == eos_token:
                    break

        return sequence, log_prob

# ================== TRAINING FUNCTION ==================

def train_char_rnn_pretrain(smiles_list: List[str], epochs: int = 5,
                          batch_size: int = 64, learning_rate: float = 1e-3,
                          ckpt_path: str = "pretrained_agent.pt",
                          vocab_path: str = "vocabulary.pkl",
                          use_simple_rnn: bool = True) -> nn.Module:
    """
    Pre-train character-level RNN on SMILES data.

    Args:
        smiles_list: List of SMILES strings
        epochs: Number of training epochs
        batch_size: Batch size
        learning_rate: Learning rate
        ckpt_path: Path to save model checkpoint
        vocab_path: Path to save vocabulary
        use_simple_rnn: Use simple RNN implementation (no cuDNN issues)

    Returns:
        Trained model
    """

    print(f"üöÄ Starting pre-training with {'Simple RNN' if use_simple_rnn else 'PyTorch RNN'}...")

    # Create vocabulary
    char_to_idx, idx_to_char = create_vocabulary(smiles_list)
    vocab_size = len(char_to_idx)

    # Save vocabulary
    with open(vocab_path, 'wb') as f:
        pickle.dump({'char_to_idx': char_to_idx, 'idx_to_char': idx_to_char}, f)
    print(f"üíæ Vocabulary saved to {vocab_path}")

    # Create dataset and dataloader
    dataset = SMILESDataset(smiles_list, char_to_idx)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,
                          num_workers=0, pin_memory=True if DEVICE == 'cuda' else False)  # num_workers=0 for stability

    # Initialize model
    if use_simple_rnn:
        model = SimpleRNN(vocab_size=vocab_size, embed_size=256, hidden_size=512,
                         num_layers=2, dropout=0.3)  # Reduced layers for stability
        print("üìö Using Simple RNN architecture (cuDNN-free)")
    else:
        # Keep original but with cuDNN disabled
        model = CharGRU(vocab_size=vocab_size, embed_size=256, hidden_size=512,
                       num_layers=2, dropout=0.3)  # Reduced layers
        print("üìö Using GRU architecture (cuDNN disabled)")

    model.to(DEVICE)
    print(f"üîß Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    # Loss and optimizer
    criterion = nn.CrossEntropyLoss(ignore_index=char_to_idx['<PAD>'])
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',
                                                          factor=0.8, patience=2)

    # Training loop
    train_losses = []

    for epoch in range(epochs):
        model.train()
        epoch_loss = 0.0
        num_batches = 0

        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")

        for batch_idx, (input_seq, target_seq) in enumerate(pbar):
            input_seq = input_seq.to(DEVICE)
            target_seq = target_seq.to(DEVICE)

            # Forward pass
            optimizer.zero_grad()
            logits, _ = model(input_seq)

            # Calculate loss
            loss = criterion(logits.reshape(-1, vocab_size), target_seq.reshape(-1))

            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            # Update metrics
            epoch_loss += loss.item()
            num_batches += 1

            # Update progress bar
            pbar.set_postfix({'Loss': f"{loss.item():.4f}",
                            'Avg Loss': f"{epoch_loss/num_batches:.4f}"})

            # Sample validation every 1000 batches
            if batch_idx % 1000 == 0 and batch_idx > 0:
                try:
                    sample_smiles = sample_molecules(model, char_to_idx, idx_to_char,
                                                   num_samples=3, device=DEVICE)
                    print(f"\nüß™ Sample molecules: {sample_smiles}")
                except Exception as e:
                    print(f"\n‚ö†Ô∏è Sampling error: {e}")
                    # Continue training even if sampling fails

        # End of epoch
        avg_loss = epoch_loss / num_batches
        train_losses.append(avg_loss)
        scheduler.step(avg_loss)

        print(f"üìä Epoch {epoch+1} - Average Loss: {avg_loss:.4f}")

        # Save checkpoint
        if (epoch + 1) % 2 == 0:
            checkpoint = {
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': epoch,
                'loss': avg_loss,
                'vocab_size': vocab_size
            }
            torch.save(checkpoint, f"checkpoint_epoch_{epoch+1}.pt")

    # Save final model
    final_checkpoint = {
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': epochs,
        'loss': train_losses[-1],
        'vocab_size': vocab_size,
        'model_config': {
            'vocab_size': vocab_size,
            'embed_size': 256,
            'hidden_size': 512,
            'num_layers': 2,
            'dropout': 0.3,
            'model_type': 'SimpleRNN' if use_simple_rnn else 'GRU'
        }
    }
    torch.save(final_checkpoint, ckpt_path)

    # Plot training curve
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses)
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.savefig('training_curve.png')
    plt.show()

    print(f"‚úÖ Pre-training complete! Model saved to {ckpt_path}")

    return model

# ================== SAMPLING FUNCTIONS ==================

def sample_molecules(model: nn.Module, char_to_idx: Dict[str, int],
                    idx_to_char: Dict[int, str], num_samples: int = 5,
                    temperature: float = 1.0, device: str = 'cpu') -> List[str]:
    """Sample molecules from trained model."""

    model.eval()
    sampled_smiles = []

    sos_idx = char_to_idx['<SOS>']
    eos_idx = char_to_idx['<EOS>']

    for _ in range(num_samples):
        sequence, _ = model.sample(sos_idx, eos_idx, temperature=temperature, device=device)

        # Convert to SMILES
        chars = []
        for idx in sequence[1:]:  # Skip SOS
            if idx == eos_idx:
                break
            chars.append(idx_to_char.get(idx, '<UNK>'))

        smiles = ''.join(chars)
        sampled_smiles.append(smiles)

    return sampled_smiles

def evaluate_model(model: nn.Module, char_to_idx: Dict[str, int],
                  idx_to_char: Dict[int, str], num_samples: int = 1000) -> Dict[str, float]:
    """Evaluate model by sampling and checking validity."""

    print("üî¨ Evaluating model...")

    sampled_smiles = sample_molecules(model, char_to_idx, idx_to_char,
                                    num_samples=num_samples, device=DEVICE)

    # Check validity
    valid_count = 0
    unique_count = len(set(sampled_smiles))

    for smiles in sampled_smiles:
        try:
            mol = Chem.MolFromSmiles(smiles)
            if mol is not None:
                valid_count += 1
        except:
            continue

    validity = valid_count / num_samples
    uniqueness = unique_count / num_samples

    print(f"üìà Evaluation Results:")
    print(f"  Validity: {validity:.3f} ({valid_count}/{num_samples})")
    print(f"  Uniqueness: {uniqueness:.3f} ({unique_count}/{num_samples})")

    return {
        'validity': validity,
        'uniqueness': uniqueness,
        'valid_count': valid_count,
        'unique_count': unique_count,
        'total_samples': num_samples
    }

# ================== MAIN EXECUTION ==================

if __name__ == "__main__":
    # Configuration
    CHEMBL_FILE = "chembl_35_chemreps.txt"
    MAX_LENGTH = 150
    MIN_LENGTH = 6
    EPOCHS = 5
    BATCH_SIZE = 64
    LEARNING_RATE = 1e-3

    # Check if file exists
    if not os.path.exists(CHEMBL_FILE):
        print(f"‚ùå Error: {CHEMBL_FILE} not found!")
        print("Please ensure the ChEMBL dataset file is in the current directory.")
        exit(1)

    print("üß¨ Starting ChEMBL Pre-training Pipeline")
    print("=" * 50)

    # Load and filter data
    filtered_smiles = load_and_filter_chembl(CHEMBL_FILE, MAX_LENGTH, MIN_LENGTH)

    if len(filtered_smiles) < 1000:
        print(f"‚ö†Ô∏è Warning: Only {len(filtered_smiles)} valid SMILES found. Consider adjusting filters.")

    # Start pre-training
    print("üöÄ Starting the pre-training of the generative agent...")
    print("NOTE: This is the most time-consuming part.")
    print("Ensure your environment has GPU access for faster training.")

    # Train the model (using Simple RNN to completely avoid cuDNN issues)
    model = train_char_rnn_pretrain(
        smiles_list=filtered_smiles,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        learning_rate=LEARNING_RATE,
        ckpt_path="pretrained_agent.pt",
        use_simple_rnn=True  # Use simple RNN to avoid all cuDNN issues
    )

    print("‚úÖ Pre-training complete! Model saved to 'pretrained_agent.pt'")

    # Evaluate the model
    with open('vocabulary.pkl', 'rb') as f:
        vocab_data = pickle.load(f)
        char_to_idx = vocab_data['char_to_idx']
        idx_to_char = vocab_data['idx_to_char']

    eval_results = evaluate_model(model, char_to_idx, idx_to_char, num_samples=1000)

    # Sample some molecules
    print("\nüß™ Sample generated molecules:")
    samples = sample_molecules(model, char_to_idx, idx_to_char, num_samples=10, device=DEVICE)
    for i, smiles in enumerate(samples, 1):
        try:
            mol = Chem.MolFromSmiles(smiles)
            valid = "‚úÖ" if mol is not None else "‚ùå"
            print(f"{i:2d}. {valid} {smiles}")
        except:
            print(f"{i:2d}. ‚ùå {smiles}")

    print("\nüéâ Pre-training pipeline completed successfully!")
    print(f"üìÅ Files created:")
    print(f"  - pretrained_agent.pt (model checkpoint)")
    print(f"  - vocabulary.pkl (character vocabulary)")
    print(f"  - training_curve.png (loss curve)")

!pip install -q selfies rdkit torch torchvision torchaudio tqdm matplotlib

# --- Fixed patch: robust build_selfies_vocab + resume training (corrected) ---
import selfies as sf
import os
import pickle
from typing import List, Tuple, Dict

print("Patching build_selfies_vocab to be robust across selfies versions...")

def build_selfies_vocab(selfies_list: List[str]) -> Tuple[Dict[str, int], Dict[int, str]]:
    """
    Robustly build a SELFIES token vocabulary.
    Tries multiple selfies API calls, falls back to tokens found in selfies_list.
    """
    # Try library-provided alphabets (various versions)
    lib_tokens = None
    try:
        lib_tokens = list(sf.get_alphabet())
    except Exception:
        try:
            lib_tokens = list(sf.get_semantic_alphabet())
        except Exception:
            try:
                lib_tokens = list(sf.get_alphabet_from_selfies())
            except Exception:
                lib_tokens = None

    if lib_tokens:
        lib = sorted(set(lib_tokens))
    else:
        # Fallback: derive tokens from dataset
        seen = set()
        for s in selfies_list:
            try:
                toks = sf.split_selfies(s)
            except Exception:
                continue
            seen.update(toks)
        lib = sorted(seen)

    # Build token list with special tokens first
    special = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']
    tokens = special + lib

    # Deduplicate preserving order
    seen_tokens = []
    seen_set = set()
    for t in tokens:
        if t not in seen_set:
            seen_tokens.append(t)
            seen_set.add(t)

    token_to_idx = {t: i for i, t in enumerate(seen_tokens)}
    idx_to_token = {i: t for t, i in token_to_idx.items()}
    return token_to_idx, idx_to_token

# Install patched function into notebook globals so earlier code will use it
globals()['build_selfies_vocab'] = build_selfies_vocab
print("Patched build_selfies_vocab installed.")

# Find selfies_list or smiles_list in the environment (from earlier cells)
selfies_list = globals().get('selfies_list', None)
smiles_list = globals().get('smiles_list', None)

if selfies_list is not None:
    print(f"Found selfies_list in environment with {len(selfies_list)} items ‚Äî will build vocab from it.")
elif smiles_list is not None:
    print(f"Found smiles_list in environment with {len(smiles_list)} items ‚Äî will convert to SELFIES during training.")
else:
    # Try to load from file 'chembl_35_chemreps.txt' or similar
    fallback_file = 'chembl_35_chemreps.txt'
    if os.path.exists(fallback_file):
        print(f"No selfies_list/smiles_list in memory; reading SMILES from {fallback_file} (first column).")
        tmp = []
        import pandas as pd
        try:
            df = pd.read_csv(fallback_file, sep='\t', low_memory=False)
            if df.shape[1] > 0:
                tmp = df.iloc[:,0].dropna().astype(str).tolist()
        except Exception:
            with open(fallback_file, 'r') as f:
                tmp = [line.strip().split('\t')[0] for line in f if line.strip()]
        smiles_list = tmp
        print(f"Loaded {len(smiles_list)} SMILES from {fallback_file}.")
    else:
        raise RuntimeError("No selfies_list or smiles_list found in the environment and CHEMBL file not present. "
                           "If you already converted SMILES->SELFIES earlier, ensure 'selfies_list' variable is in scope, "
                           "or upload the CHEMBL file and set CHEMBL_FILE accordingly.")

# Get the train function from the notebook
train_fn = globals().get('train_selfies_rnn', None)
if train_fn is None:
    raise RuntimeError("train_selfies_rnn is not defined in the notebook. Make sure you have the SELFIES generator code cell run earlier.")

# Training hyperparams (same defaults you used before)
EPOCHS = 5
BATCH_SIZE = 64
LR = 1e-3
CKPT = 'pretrained_selfies_agent.pt'

print("Starting (resuming) training with train_selfies_rnn(...) ‚Äî this may take a while depending on epochs and data size.")
if smiles_list is not None:
    # call with SMILES list (function will convert -> SELFIES internally)
    model, token_to_idx, idx_to_token = train_fn(smiles_list, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR, ckpt_path=CKPT)
else:
    # If only selfies_list exists, decode to smiles and call train_fn
    try:
        decoded = []
        for s in selfies_list:
            smi = sf.decoder(s)
            if smi:
                decoded.append(smi)
        print(f"Decoded {len(decoded)} SMILES from existing selfies_list; passing to train_selfies_rnn for training.")
        model, token_to_idx, idx_to_token = train_fn(decoded, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR, ckpt_path=CKPT)
    except Exception as e:
        raise RuntimeError(f"Couldn't fall back to train_selfies_rnn with decoded SMILES: {e}")

# Save vocab (redundant but safe)
with open('vocabulary_selfies.pkl', 'wb') as f:
    pickle.dump({'token_to_idx': token_to_idx, 'idx_to_token': idx_to_token}, f)
print("Saved vocabulary_selfies.pkl (token_to_idx, idx_to_token).")
print("Vocab size:", len(token_to_idx))
print("First 20 tokens:", list(token_to_idx.keys())[:20])

# Quick sample & evaluation if function exists
sample_and_evaluate = globals().get('sample_and_evaluate', None)
if sample_and_evaluate is not None:
    print("Running quick sample_and_evaluate (n=200) to sanity-check validity...")
    try:
        metrics = sample_and_evaluate(model, token_to_idx, idx_to_token, n_samples=200)
        print("Sanity metrics:", metrics)
    except Exception as e:
        print("sample_and_evaluate failed:", e)

print("Patch & resume completed.")