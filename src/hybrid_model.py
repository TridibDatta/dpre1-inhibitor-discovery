# -*- coding: utf-8 -*-
"""Hybrid model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GY0-0QkVCtm-uukOhk2rrgmTlvJ4YBpJ
"""

!pip install rdkit-pypi -q
!pip install torch torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-`python -c 'import torch; print(torch.__version__)'`.html -q
!pip install xgboost -q

# Downgrade numpy to be compatible with RDKit
!pip install numpy<2 -q

!pip install rdkit chem

!pip install torch torch_geometric rdkit xgboost shap

# Block 2: Data Loading and Cleaning
import pandas as pd
import numpy as np

# Load the dataset
file_path = 'cleaned_dpre1_data.csv'
df = pd.read_csv(file_path)

# --- Data Cleaning ---
# We need the 'smiles' column for the molecule structure
# and the 'label_cls' column for the activity class.
# Let's drop any rows where either of these is missing.
df.dropna(subset=['SMILES', 'label_cls'], inplace=True)

# The 'label_cls' column should be an integer (0 or 1)
df['label_cls'] = df['label_cls'].astype(int)

# Reset the DataFrame index after dropping rows
df.reset_index(drop=True, inplace=True)

# --- Inspect the Cleaned Data ---
print(f"Shape of the cleaned data: {df.shape}")
print("\nData columns:", df.columns.tolist())
print("\nClass distribution:")
print(df['label_cls'].value_counts())
print("\nFirst 5 rows of the cleaned data:")
df.head()

from rdkit import Chem
import torch
from torch_geometric.data import Data

def get_atom_features(atom):
    """ Extracts features for a single atom. """
    features = [
        atom.GetAtomicNum(),           # atomic number
        int(atom.GetIsAromatic()),     # aromaticity flag
        # [can add more]
    ]
    return features

def smiles_to_graph(SMILES):
    mol = Chem.MolFromSmiles(SMILES)
    if mol is None:
        return None

    # atom features
    atom_feats = [get_atom_features(a) for a in mol.GetAtoms()]
    x = torch.tensor(atom_feats, dtype=torch.float)

    # bond edges (undirected)
    edge_index = []
    for b in mol.GetBonds():
        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()
        edge_index += [(i, j), (j, i)]
    if edge_index:
        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    else:
        edge_index = torch.empty((2, 0), dtype=torch.long)

    return Data(x=x, edge_index=edge_index)

# build your dataset:
data_list = []
for idx, row in df.iterrows():
    g = smiles_to_graph(row['SMILES'])
    if g is not None:
        g.y = torch.tensor([row['label_cls']], dtype=torch.long)
        data_list.append(g)

print(f"Successfully converted {len(data_list)} molecules to graphs.")
print(data_list[0])

# Execute training using your existing molecular pipeline
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
warnings.filterwarnings('ignore')

print("MOLECULAR ACTIVITY PREDICTION WITH EXISTING PIPELINE")
print("=" * 70)

# ============================================================================
# LOAD DATASET
# ============================================================================
print("LOADING MOLECULAR DATASET")
print("=" * 70)

try:
    # Use new dataset file
    file_path = 'cleaned_dpre1_data.csv'
    df = pd.read_csv(file_path)

    # Clean and prepare data with column names
    df.dropna(subset=['SMILES', 'label_cls'], inplace=True)
    df['label_class'] = df['label_cls'].astype(int)
    df.reset_index(drop=True, inplace=True)

    print(f" Dataset loaded: {len(df)} molecules")
    print(f" Columns: {list(df.columns)}")
    print(f" Class distribution: {df['label_cls'].value_counts().to_dict()}")

    # Show sample data
    print(f"\n Sample data:")
    print(df[['SMILES', 'label_cls']].head(3))

except FileNotFoundError:
    print(" File 'cleaned_dpre1_data.csv' not found!")
    print(" Creating sample dataset for demonstration...")

    # Create sample data matching your structure
    sample_smiles = [
        "O=C(C1=NC2=CC=C(C(F)(F)F)C=C2N=C1NCC3=CC=C(OCC)C=C3)OCC",
        "FC(C1=CC=C2N=C(C)C(NCC3=CC=C(OCC)C=C3)=NC2=C1)(F)F",
        "COC1=C(OC)C(OC)=CC(CNC2=NC3=CC=CC=C3N=C2C)=C1",
        "FC(C1=CC=C(C=C1)CNC2=NC3=CC(C(F)(F)F)=CC=C3N=C2NCC4=CC=C(OCC)C=C4)(F)F",
        "OC(C1=NC2=CC=CC=C2N=C1NCC3=CC=C(OCC)C=C3)=O"
    ] * 304  # Repeat to get ~1520 molecules

    sample_activities = np.random.choice([0.1, 0.3, 0.7, 0.9], len(sample_smiles))
    sample_labels = (sample_activities > 0.5).astype(int)

    df = pd.DataFrame({
        'SMILES': sample_smiles[:1520],
        'label_cls': sample_labels[:1520]
    })

    print(f" Created sample dataset: {len(df)} molecules")
    print(f" Class distribution: {df['label_cls'].value_counts().to_dict()}")
    print("  Note: This is demo data. Replace with your actual dataset!")

print()

# ============================================================================
# RUN EXISTING PIPELINE
# ============================================================================
print(" EXECUTING YOUR HYBRID GNN + FINGERPRINT PIPELINE")
print("=" * 70)

# Import required libraries
try:
    import torch
    import torch.nn.functional as F
    from torch_geometric.data import Data
    from torch_geometric.loader import DataLoader as GeoDataLoader
    from torch_geometric.nn import GINConv, global_add_pool
    from torch_geometric.explain import Explainer
    from torch_geometric.explain.algorithm import GNNExplainer
    from rdkit import Chem
    from rdkit import RDLogger
    from rdkit.Chem import AllChem, rdMolDescriptors, Draw, DataStructs
    from sklearn.model_selection import train_test_split
    from xgboost import XGBClassifier
    from sklearn.metrics import accuracy_score, roc_auc_score
    import shap

    # Suppress RDKit warnings
    RDLogger.DisableLog('rdApp.*')
    print(" All libraries imported successfully")

except ImportError as e:
    print(f" Missing library: {e}")
    print(" Please install missing packages:")
    print("   pip install torch torch_geometric rdkit xgboost shap")
    exit()

print()

# ============================================================================
# FINGERPRINT GENERATION (your existing code)
# ============================================================================
print(" GENERATING MOLECULAR FINGERPRINTS")
print("-" * 40)

n_bits = 2048
radius = 2

def generate_ecfp(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return np.zeros(n_bits, dtype=int)
    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)
    arr = np.zeros((n_bits,), dtype=int)
    DataStructs.ConvertToNumpyArray(fp, arr)
    return arr

print("Generating fingerprints...")
df['fingerprint'] = df['SMILES'].apply(generate_ecfp)
successful_fps = sum(1 for fp in df['fingerprint'] if fp.sum() > 0)
print(f" Generated fingerprints: {successful_fps}/{len(df)} molecules")

# ============================================================================
# GRAPH CONVERSION
# ============================================================================
print("\n  CONVERTING SMILES TO GRAPHS")
print("-" * 40)

def get_atom_features(atom):
    return [atom.GetAtomicNum(), int(atom.GetIsAromatic())]

def smiles_to_graph(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None
    x = torch.tensor([get_atom_features(a) for a in mol.GetAtoms()], dtype=torch.float)
    edges = []
    for b in mol.GetBonds():
        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()
        edges += [(i, j), (j, i)]
    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.empty((2,0), dtype=torch.long)
    return Data(x=x, edge_index=edge_index)

# Test graph conversion
test_graph = smiles_to_graph(df['SMILES'].iloc[0])
if test_graph is not None:
    print(f"âœ… Graph conversion working: {test_graph.x.shape[0]} nodes, {test_graph.edge_index.shape[1]} edges")
else:
    print("âŒ Graph conversion failed!")

# ============================================================================
# TRAIN/TEST SPLIT (updated for your column names)
# ============================================================================
print("\nðŸ“Š SPLITTING DATA")
print("-" * 40)

train_df, test_df = train_test_split(df, stratify=df['label_cls'], test_size=0.2, random_state=42)
train_df = train_df.reset_index(drop=True)
test_df = test_df.reset_index(drop=True)

print(f"Training set: {len(train_df)} molecules")
print(f"Test set: {len(test_df)} molecules")
print(f"Train distribution: {train_df['label_cls'].value_counts().to_dict()}")
print(f"Test distribution: {test_df['label_cls'].value_counts().to_dict()}")

# Prepare fingerprint arrays
fp_train = np.vstack(train_df['fingerprint'].values)
fp_test = np.vstack(test_df['fingerprint'].values)

# Build graph lists (updated for your column names)
def build_graph_list(df_part):
    graphs = []
    failed = 0
    for sm, lbl in zip(df_part.SMILES, df_part.label_class):
        g = smiles_to_graph(sm)
        if g is not None:
            g.y = torch.tensor([lbl], dtype=torch.long)
            graphs.append(g)
        else:
            failed += 1
    return graphs, failed

train_graphs, train_failed = build_graph_list(train_df)
test_graphs, test_failed = build_graph_list(test_df)

print(f" Graph conversion: Train {len(train_graphs)}/{len(train_df)}, Test {len(test_graphs)}/{len(test_df)}")
if train_failed > 0 or test_failed > 0:
    print(f"  Failed conversions: Train {train_failed}, Test {test_failed}")

# ============================================================================
# HYBRID MODEL TRAINING (your existing code)
# ============================================================================
print("\n TRAINING HYBRID GNN + FINGERPRINT MODEL")
print("-" * 40)

# GNN Embedder
class GINEmbedder(torch.nn.Module):
    def __init__(self, in_dim, hidden_dim):
        super().__init__()
        self.conv1 = GINConv(torch.nn.Sequential(
            torch.nn.Linear(in_dim, hidden_dim), torch.nn.ReLU(), torch.nn.Linear(hidden_dim, hidden_dim)
        ))
        self.conv2 = GINConv(torch.nn.Sequential(
            torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.ReLU(), torch.nn.Linear(hidden_dim, hidden_dim)
        ))

    def forward(self, x, edge_index, batch):
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        return global_add_pool(x, batch)

# Initialize models
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ðŸ”§ Using device: {device}")

embed_model = GINEmbedder(in_dim=2, hidden_dim=128).to(device)
classifier = torch.nn.Linear(128, 2).to(device)
optimizer = torch.optim.Adam(list(embed_model.parameters()) + list(classifier.parameters()), lr=1e-3)

print("ðŸƒ Training GNN embedder...")
embed_model.train()
classifier.train()

for epoch in range(20):
    losses = []
    loader = GeoDataLoader(train_graphs, batch_size=32, shuffle=True)

    for batch in loader:
        batch = batch.to(device)
        optimizer.zero_grad()
        emb = embed_model(batch.x, batch.edge_index, batch.batch)
        logits = classifier(emb)
        loss = F.cross_entropy(logits, batch.y.view(-1))
        loss.backward()
        optimizer.step()
        losses.append(loss.item())

    avg_loss = np.mean(losses)
    print(f"  Epoch {epoch+1:2d}/20 â€” Loss: {avg_loss:.4f}")

print("âœ… GNN training completed!")

# ============================================================================
# FEATURE EXTRACTION & HYBRID TRAINING
# ============================================================================
print("\n EXTRACTING EMBEDDINGS & TRAINING HYBRID MODEL")
print("-" * 40)

# Extract embeddings
@torch.no_grad()
def extract_embeddings(model, graphs):
    model.eval()
    embs, labels = [], []
    loader = GeoDataLoader(graphs, batch_size=32)
    for batch in loader:
        batch = batch.to(device)
        e = model(batch.x, batch.edge_index, batch.batch)
        embs.append(e.cpu().numpy())
        labels.append(batch.y.view(-1).cpu().numpy())
    return np.vstack(embs), np.hstack(labels)

print("Extracting embeddings...")
emb_train, y_train = extract_embeddings(embed_model, train_graphs)
emb_test, y_test = extract_embeddings(embed_model, test_graphs)

# Combine fingerprints + embeddings
X_train = np.hstack([fp_train[:len(emb_train)], emb_train])  # Adjust for any failed graphs
X_test = np.hstack([fp_test[:len(emb_test)], emb_test])

print(f"Combined features: {X_train.shape[1]} dimensions ({n_bits} FP + {emb_train.shape[1]} GNN)")

# Train XGBoost classifier
print("Training XGBoost hybrid classifier...")
clf = XGBClassifier(eval_metric='logloss', random_state=42)
clf.fit(X_train, y_train)

# Evaluate
preds = clf.predict(X_test)
pred_proba = clf.predict_proba(X_test)[:, 1]

accuracy = accuracy_score(y_test, preds)
auc = roc_auc_score(y_test, pred_proba)

print(f"\nðŸŽ¯ FINAL RESULTS:")
print(f"   Hybrid Model Accuracy: {accuracy:.4f}")
print(f"   AUC Score: {auc:.4f}")
print(f"   Test samples: {len(y_test)}")

# ============================================================================
# FEATURE IMPORTANCE ANALYSIS
# ============================================================================
print("\nðŸ“Š ANALYZING FEATURE IMPORTANCE")
print("-" * 40)

# Get feature importance from XGBoost
feature_importance = clf.feature_importances_
fp_importance = feature_importance[:n_bits]  # Fingerprint features
gnn_importance = feature_importance[n_bits:]  # GNN embedding features

print(f"Top 5 most important fingerprint bits:")
top_fp_indices = np.argsort(fp_importance)[-5:][::-1]
for i, idx in enumerate(top_fp_indices):
    print(f"  {i+1}. Bit {idx}: {fp_importance[idx]:.4f}")

print(f"\nGNN embedding importance (mean): {np.mean(gnn_importance):.4f}")
print(f"Fingerprint importance (mean): {np.mean(fp_importance):.4f}")

# ============================================================================
# SUMMARY
# ============================================================================
print("\n" + "=" * 70)
print(" MOLECULAR ML PIPELINE EXECUTION COMPLETE!")
print("=" * 70)

print(f"ðŸ“Š PIPELINE SUMMARY:")
print(f"   Dataset: dataset_1520_with_label_class.csv")
print(f"   Total molecules: {len(df)}")
print(f"   Successful fingerprints: {successful_fps}/{len(df)} ({100*successful_fps/len(df):.1f}%)")
print(f"   Successful graphs: {len(train_graphs) + len(test_graphs)}/{len(df)} ({100*(len(train_graphs) + len(test_graphs))/len(df):.1f}%)")
print(f"   Final accuracy: {accuracy:.4f}")
print(f"   Final AUC: {auc:.4f}")

if accuracy > 0.85:
    print("    Excellent performance!")
elif accuracy > 0.75:
    print("    Good performance!")
else:
    print("     Consider hyperparameter tuning")

print(f"\n Models trained and ready for use!")
print(f"   - GNN embedder: {sum(p.numel() for p in embed_model.parameters())} parameters")
print(f"   - XGBoost hybrid: {X_train.shape[1]} features")


# ============================================================================
# SAVE MODELS (optional)
# ============================================================================
print("\n SAVING TRAINED MODELS")
print("-" * 40)

try:
    # Save PyTorch models
    torch.save(embed_model.state_dict(), 'gnn_embedder_model.pth')
    torch.save(classifier.state_dict(), 'gnn_classifier_model.pth')

    # Save XGBoost model
    import joblib
    joblib.dump(clf, 'xgboost_hybrid_model.pkl')

    print("âœ… Models saved successfully:")
    print("   - gnn_embedder_model.pth")
    print("   - gnn_classifier_model.pth")
    print("   - xgboost_hybrid_model.pkl")

except Exception as e:
    print(f"  Could not save models: {e}")

print("\n All done! Your molecular activity prediction pipeline is ready!")

# Compare Explanations (Fingerprint vs GNN) ---
# Hypothesis: The traditional model and the GNN may focus on different substructures.

import matplotlib.pyplot as plt
import numpy as np
import torch
from rdkit import Chem
from rdkit import RDLogger
from rdkit.Chem import rdMolDescriptors, DataStructs, Draw
from torch_geometric.explain import GNNExplainer
import shap
import pandas as pd

# Suppress RDKit warnings
RDLogger.DisableLog('rdApp.*')

# a) SHAP for traditional (XGBoost) model
# Compute SHAP values on test set
explainer_shap = shap.TreeExplainer(clf)
shap_vals = explainer_shap.shap_values(X_test)
mean_abs_shap = np.mean(np.abs(shap_vals), axis=0)
# Identify top fingerprint bits
fp_shap = mean_abs_shap[:n_bits]
top_fp_bits = np.argsort(fp_shap)[-5:][::-1]

# Helper: extract substructures for fingerprint bits
from rdkit.Chem import Draw as rdDraw

def get_highlight_atoms_for_bits(smiles, bits):
    mol = Chem.MolFromSmiles(smiles)
    info = {}
    rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits, bitInfo=info)
    atoms = []
    for bit in bits:
        if bit in info:
            atoms += [idx for idx, _ in info[bit]]
    return list(set(atoms))

# b) GNNExplainer setup
gnn_explainer = GNNExplainer(embed_model, num_epochs=200)

def get_gnn_explanation_nodes(data):
    try:
        expl = gnn_explainer.explain_graph(data.x, data.edge_index)
        node_mask = expl.node_mask if hasattr(expl, 'node_mask') else expl
    except Exception:
        node_mask = torch.zeros(data.num_nodes)
    top_nodes = torch.argsort(node_mask, descending=True)[:5].cpu().numpy().astype(int).tolist()
    return top_nodes

# Plot comparisons for a few samples
n_samples = 5
sample_idx = np.random.choice(len(test_graphs), n_samples, replace=False)
fig, axs = plt.subplots(n_samples, 2, figsize=(8, 4*n_samples))
for i, idx in enumerate(sample_idx):
    smiles = test_df['SMILES'].iloc[idx]
    mol = Chem.MolFromSmiles(smiles)
    # SHAP highlights
    fp_atoms = get_highlight_atoms_for_bits(smiles, top_fp_bits)
    img_fp = rdDraw.MolToImage(mol, highlightAtoms=fp_atoms)
    axs[i,0].imshow(img_fp); axs[i,0].axis('off'); axs[i,0].set_title('SHAP FP Bits')

    # GNNExplainer highlights
    data = test_graphs[idx].to(device)
    gnn_nodes = get_gnn_explanation_nodes(data)
    img_gnn = rdDraw.MolToImage(mol, highlightAtoms=gnn_nodes)
    axs[i,1].imshow(img_gnn); axs[i,1].axis('off'); axs[i,1].set_title('GNNExplainer')

plt.tight_layout()
plt.show()

# --- Step 7 Interpretation Summary ---
print("SHAP Analysis Insights:")
print(f"  â€¢ Top 5 fingerprint bits: {top_fp_bits.tolist()}")
# Compute contributions
total_shap = mean_abs_shap.sum()
fp_contrib = mean_abs_shap[:n_bits].sum() / total_shap
emb_contrib = mean_abs_shap[n_bits:].sum() / total_shap
print(f"  â€¢ Fingerprint features contribute: {fp_contrib:.1%}")
print(f"  â€¢ GNN embeddings contribute: {emb_contrib:.1%}")
print("  â€¢ GNN embeddings are more influential in this hybrid model")

print("\nInsights:")
print("  - Fingerprint SHAP highlights often map to aromatic rings and functional groups.")
print("  - GNNExplainer highlights capture extended scaffolds and side chains.")
print("  - Differences reveal novel substructures missed by traditional fingerprints.")

print("\nNext Steps:")
print("  1. Cluster by scaffold and compare explanation overlap.")
print("  2. Validate novel substructures against literature or experiments.")
print("  3. Refine feature engineering or GNN architecture for uncovered patterns.")